# Tuned Lens

The Tuned Lens is a technique for understanding how transformer predictions are built layer-by-layer that improves upon nostalgebraist's [logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). The Tuned Lens is more robust, less biased, and applicable to more models than the Logit Lens.

Originally concieved by [Igor Ostrovsky](https://twitter.com/igoro?lang=en) and [Stella Biderman](www.stellabiderman) at [EleutherAI](www.eleuther.ai), this library was built as a colaboration between FAR and EleutherAI researchers. This library provides the means to replicate our forthcoming paper "Eliciting Latent Predictions from Transformers with the Tuned Lens" and additionally provides useful tooling for researchers interested in experimenting with depth-based probing.

<img width="1028" alt="tuned lens" src="https://user-images.githubusercontent.com/39116809/206883419-4fb9083d-3fa0-48e9-ba97-b70cb21b08e9.png">

## Quick Start

You can install this library from source via `pip install git+https://github.com/AlignmentResearch/tuned-lens`, or by cloning it and running `pip install .`

## Replicating Belrose et al. (2023)

### Experiment 1

### Experiment 2

### Experiment 3

### Experiment 4

## Additional Uses

## Extending the Library

## Citation Information

If you find this library userful, please cite it as

```bibtex
@article{belrose2023eliciting,
  title={Eliciting Latent Predictions from Transformers with the Tuned Lens},
  authors={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and McKinney, Lev and Ostrovsky, Igor and Biderman, Stella and Steinhardt, Jacob},
  journal={to appear},
  year={2023}
}
```
