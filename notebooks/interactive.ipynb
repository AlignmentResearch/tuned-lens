{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tuned lens ðŸ”Ž\n",
    "A tuned lens allows us to peak at the iterative computations that a transformer is using the compute the next token.\n",
    "\n",
    "A lens into a transformer with n layers allows you to replace the last $m$ layers of the model with an [affine transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (we call these affine adapters).\n",
    "\n",
    "This essentially skips over these last few layers and lets you see the best prediction that can be made from the model's representations, i.e. the residual stream, at layer $n - m$. Since the representations may be rotated, shifted, or stretched from layer to layer it's useful to train the len's affine adapters specifically on each layer. This training is what differentiates this method from simpler approaches that decode the residual stream of the network directly using the unembeding layer i.e. the logit lens. We explain this process in more detail in a forthcoming paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TunedLens.load: ignoring config key 'sublayers'\n",
      "TunedLens.load: ignoring config key 'orthogonal'\n",
      "TunedLens.load: ignoring config key 'rank'\n",
      "TunedLens.load: ignoring config key 'include_final'\n",
      "TunedLens.load: ignoring config key 'identity_init'\n"
     ]
    }
   ],
   "source": [
    "from tuned_lens.nn.lenses import TunedLens, LogitLens\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tuned_lens = TunedLens.load(\"gpt2\")\n",
    "logit_lens = LogitLens(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48739ab77cd04a2cb472f5d4df62d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select Lens:', options=(('Tuned Lens', TunedLens(\n",
       "  (extra_layers)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tuned_lens.plotting import plot_lens\n",
    "import ipywidgets as widgets\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "def make_plot(lens, text, statistic, token_range):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    if len(input_ids[0]) == 0:\n",
    "        return widgets.Text(\"Please enter some text.\")\n",
    "\n",
    "    return go.FigureWidget(\n",
    "        plot_lens(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            lens,\n",
    "            input_ids=input_ids,\n",
    "            start_pos=token_range[0],\n",
    "            end_pos=token_range[1] if token_range[1] > 0 else None,\n",
    "            statistic=statistic,\n",
    "        )\n",
    "    )\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "statistic_wdg = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Entropy', 'entropy'),\n",
    "        ('Cross Entropy', 'ce'), \n",
    "        ('Forward KL', 'forward_kl'),\n",
    "    ],\n",
    "    description='Select Statistic:',\n",
    "    style=style,\n",
    ")\n",
    "text_wdg = widgets.Textarea(\n",
    "    description=\"Input Text\",\n",
    "    value=\"We propose a new simple network architecture, the Transformer\",\n",
    ")\n",
    "lens_wdg = widgets.Dropdown(\n",
    "    options=[('Tuned Lens', tuned_lens), ('Logit Lens', logit_lens)],\n",
    "    description='Select Lens:',\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "token_range_wdg = widgets.IntRangeSlider(\n",
    "    description='Token Range',\n",
    "    min=0, \n",
    "    max=1, \n",
    "    step=1, \n",
    "    style=style,\n",
    ")\n",
    "\n",
    "\n",
    "def update_token_range(*args):\n",
    "    token_range_wdg.max = len(tokenizer.encode(text_wdg.value))\n",
    "\n",
    "update_token_range()\n",
    "\n",
    "token_range_wdg.value = [0, token_range_wdg.max]\n",
    "text_wdg.observe(update_token_range, 'value')\n",
    "\n",
    "interact = widgets.interact.options(manual_name='Run Lens', manual=True)\n",
    "\n",
    "plot = interact(\n",
    "    make_plot,\n",
    "    text=text_wdg,\n",
    "    statistic=statistic_wdg,\n",
    "    lens=lens_wdg,\n",
    "    token_range=token_range_wdg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca27fca65bbd5c56c827a2643e94bc7b2b551ee6ee2fe84566f2c789012bce4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
